{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Malayalam NLP - using Huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unicode_ignore_regex = r'[\\u200e\\u200c\\u200d]'\n",
    "english_ignre_regex = r'[a-zA-Z]'\n",
    "\n",
    "def remove_special_characters(batch):\n",
    "    batch[\"text\"] = batch[\"text\"].strip()\n",
    "    batch[\"text\"] =  re.sub(char_to_ignore_regex, '' , batch[\"text\"])\n",
    "    batch[\"text\"] =  re.sub(unicode_ignore_regex, '' , batch[\"text\"])\n",
    "    batch[\"text\"] =  re.sub(english_ignre_regex, '' , batch[\"text\"])\n",
    "\n",
    "content = []\n",
    "with open ('D:\\ml\\mal-txt\\\\000002_html_body.txt', 'r', encoding='UTF8', newline='' ) as f:\n",
    "    content = { \"text\": f.read() }\n",
    "    remove_special_characters(content)\n",
    "    with open ('D:\\ml\\mal-txt\\\\000002_html_body_1.txt', 'w' , encoding='UTF8', newline='') as f1:\n",
    "        f1.write(content[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "paths = [str(x) for x in Path(\"D:/ml/mal-txt\").glob(\"**/*.txt\")]\n",
    "\n",
    "#Initialize a tokenizer\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "#Customize training\n",
    "tokenizer.train(files=paths, vocab_size=10000, min_frequency=2, special_tokens=[\n",
    "    \"<s>\",\n",
    "    \"<pad>\",\n",
    "    \"</s>\",\n",
    "    \"<unk>\",\n",
    "    \"<mask>\"\n",
    "])\n",
    "\n",
    "tokenizer.save_model(\"D:/ml/Malayalam2021BERTo\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from tokenizers.processors import BertProcessing\n",
    "\n",
    "tokenizer_folder=\"D:/ml/Malayalam2021BERTo\"\n",
    "# Create the tokenizer using vocab.json and mrege.txt files\n",
    "tokenizer = ByteLevelBPETokenizer(\n",
    "    os.path.abspath(os.path.join(tokenizer_folder,'vocab.json')),\n",
    "    os.path.abspath(os.path.join(tokenizer_folder,'merges.txt'))\n",
    ")\n",
    "\n",
    "tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "    (\"<s>\", tokenizer.token_to_id(\"<s>\"))\n",
    ")\n",
    "\n",
    "tokenizer.enable_truncation(max_length=512)\n",
    "\n",
    "tokenizer.decode(tokenizer.encode(\"മുഖ്യമന്ത്രിക്കെതിരെ ജേക്കബ് തോമസ്\").ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataSet Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import RobertaTokenizerFast\n",
    "\n",
    "\n",
    "# $env:HF_DATASETS_CACHE='D:\\ml\\HF_cache'\n",
    "# $env:HF_DATASETS_CACHE\n",
    "\n",
    "\n",
    "base_url = 'D:/ml/mal-txt/'\n",
    "wiki_data_Files = []\n",
    "for i in range(2):\n",
    "    zeros = '00000'\n",
    "    if i > 9:\n",
    "        zeros = '0000'\n",
    "\n",
    "    file1 = base_url + zeros + str(i) + '_html_body.txt'\n",
    "    wiki_data_Files.append(file1)\n",
    "print(wiki_data_Files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = load_dataset('text', data_files=wiki_data_Files, split=\"train\", cache_dir=\"D:/ml/HF_cache\")\n",
    "\n",
    "dataset = dataset.map(remove_special_characters)\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained('D:\\ml\\Malayalam2021BERTo')\n",
    "\n",
    "def tokenize_function(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "print(tokenized_dataset)\n",
    "\n",
    "tokenized_dataset.save_to_disk('D:/ml/mal-dataset')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/transformers/master/notebooks.html#examples\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from datasets import load_from_disk, load_metric\n",
    "from transformers import Trainer, TrainingArguments, RobertaConfig, RobertaTokenizerFast, RobertaForMaskedLM, DataCollatorForLanguageModeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import notebook_login\n",
    "# notebook_login()\n",
    "# instead us huggingface-cli login , in terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# wandb.login()\n",
    "# wandb.init(project=\"ml-base\", \n",
    "# name=\"ml-robertaformaskedlm-lr\",\n",
    "# tags=[\"baseline\", \"ml-high-lr\"],\n",
    "# group=\"roberta\")\n",
    "# %env WANDB_PROJECT=ml-base\n",
    "# $env:WANDB_PROJECT = ml-base\n",
    "# Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"D:/ml/malayalam_2020_wiki/malayalam-wiki2021-BERTo\"\n",
    "tokenizer_checkpoint = \"D:/ml/malayalam_2020_wiki\" #\"rajeshradhakrishnan/malayalam-wiki2021-BERTo\" #rajeshradhakrishnan/malayalam-wiki2021-BERTo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print(\"Step 1. Compute Metric\")\n",
    "# # metric = load_metric(\"accuracy\")\n",
    "# def compute_metrics(eval_pred):\n",
    "#   with torch.no_grad():\n",
    "# logits, labels = eval_pred\n",
    "# predictions = np.argmax(logits, axis=-1)\n",
    "# return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_from_disk('D:/ml/mal-dataset')\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = dataset.train_test_split()\n",
    "print(dataset.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "small_train_dataset = dataset['train'].shuffle(seed=42).select(range(1000))\n",
    "small_eval_dataset = dataset['test'].shuffle(seed=42).select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained('D:/ml/Malayalam2021BERTo', max_len=512)\n",
    "\n",
    "config = RobertaConfig(\n",
    "                        vocab_size=10000,\n",
    "                        max_position_embeddings=514,\n",
    "                        num_attention_heads=12,\n",
    "                        num_hidden_layers=6,\n",
    "                        type_vocab_size=1,\n",
    "                        )\n",
    "    \n",
    "model = RobertaForMaskedLM(config=config)\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "     tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    "     )\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    output_dir=f\"{model_checkpoint}\",\n",
    "    overwrite_output_dir=True,\n",
    "    gradient_accumulation_steps=2,\n",
    "    group_by_length=True,\n",
    "    num_train_epochs=25,\n",
    "    per_device_train_batch_size=2,\n",
    "    save_steps=800,\n",
    "    save_total_limit=5,\n",
    "    eval_steps=500,\n",
    "    logging_steps=500,\n",
    "    learning_rate=2e-5,\n",
    "    warmup_steps=800,\n",
    "    prediction_loss_only=True,\n",
    "    weight_decay=0.01,\n",
    "    # report_to=\"wandb\",  \n",
    "    # enable logging to W&B\n",
    "    # run_name=\"ml-robertaformaskedlm-lr\",  \n",
    "    # name of the W&B run (optional)\n",
    "    # push_to_hub=False\n",
    "    )\n",
    "    \n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=small_train_dataset,   #train_datasets,  #small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    # compute_metrics=compute_metrics\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainer.save_model(\"D:/ml/malayalam_2020_wiki\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation - 1\n",
    "\n",
    "\n",
    "Step 1. Dataset load_from_disk 2021-11-24 16:54:10.724168\n",
    "\n",
    "Dataset({\n",
    "    features: ['attention_mask', 'input_ids', 'text'],\n",
    "    num_rows: 2393007\n",
    "})\n",
    "\n",
    "dict_keys(['train', 'test'])\n",
    "\n",
    "Step 2. Prepare small train & eval 2021-11-24 16:54:15.582255\n",
    "\n",
    "Step 3. Setup Configuration 2021-11-24 16:54:15.971297\n",
    "\n",
    "Step 4. Start Train 2021-11-24 16:54:17.568005\n",
    "\n",
    "***** Running training *****\n",
    "  Num examples = 1000\n",
    "  Num Epochs = 25\n",
    "  Instantaneous batch size per device = 2\n",
    "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
    "  Gradient Accumulation steps = 2\n",
    "  Total optimization steps = 6250\n",
    "{'eval_loss': 6.178254127502441, 'eval_runtime': 828.4684, 'eval_samples_per_second': 1.207, 'eval_steps_per_second': 0.151, 'epoch': 1.0}\n",
    "\n",
    "{'eval_loss': 4.928427219390869, 'eval_runtime': 739.8563, 'eval_samples_per_second': 1.352, 'eval_steps_per_second': 0.169, 'epoch': 2.0}\n",
    "\n",
    "{'eval_loss': 4.488035202026367, 'eval_runtime': 868.2838, 'eval_samples_per_second': 1.152, 'eval_steps_per_second': 0.144, 'epoch': 3.0}\n",
    "\n",
    "{'eval_loss': 4.229452610015869, 'eval_runtime': 787.6381, 'eval_samples_per_second': 1.27, 'eval_steps_per_second': 0.159, 'epoch': 4.0} \n",
    "\n",
    "{'eval_loss': 4.183677673339844, 'eval_runtime': 877.2295, 'eval_samples_per_second': 1.14, 'eval_steps_per_second': 0.142, 'epoch': 5.0}\n",
    "\n",
    "{'eval_loss': 4.077062129974365, 'eval_runtime': 6648.7573, 'eval_samples_per_second': 0.15, 'eval_steps_per_second': 0.019, 'epoch': 7.0}\n",
    "\n",
    "{'eval_loss': 4.095050811767578, 'eval_runtime': 897.0366, 'eval_samples_per_second': 1.115, 'eval_steps_per_second': 0.139, 'epoch': 8.0}\n",
    "\n",
    "{'eval_loss': 4.09254264831543, 'eval_runtime': 1348.2101, 'eval_samples_per_second': 0.742, 'eval_steps_per_second': 0.093, 'epoch': 9.0}\n",
    "\n",
    "{'eval_loss': 4.024544715881348, 'eval_runtime': 734.683, 'eval_samples_per_second': 1.361, 'eval_steps_per_second': 0.17, 'epoch': 10.0}\n",
    "\n",
    "{'eval_loss': 4.003402233123779, 'eval_runtime': 927.9644, 'eval_samples_per_second': 1.078, 'eval_steps_per_second': 0.135, 'epoch': 12.0}\n",
    "\n",
    "{'eval_loss': 3.8958804607391357, 'eval_runtime': 932.9886, 'eval_samples_per_second': 1.072, 'eval_steps_per_second': 0.134, 'epoch': 13.0} \n",
    "\n",
    "{'eval_loss': 3.8974709510803223, 'eval_runtime': 945.0181, 'eval_samples_per_second': 1.058, 'eval_steps_per_second': 0.132, 'epoch': 14.0}\n",
    "\n",
    "{'eval_loss': 3.942619562149048, 'eval_runtime': 10342.7618, 'eval_samples_per_second': 0.097, 'eval_steps_per_second': 0.012, 'epoch': 15.0}\n",
    "\n",
    "{'eval_loss': 3.9202730655670166, 'eval_runtime': 788.6991, 'eval_samples_per_second': 1.268, 'eval_steps_per_second': 0.158, 'epoch': 16.0}\n",
    "\n",
    "{'eval_loss': 3.859833240509033, 'eval_runtime': 5373.0515, 'eval_samples_per_second': 0.186, 'eval_steps_per_second': 0.023, 'epoch': 17.0}\n",
    "\n",
    "{'eval_loss': 3.856739044189453, 'eval_runtime': 44248.3049, 'eval_samples_per_second': 0.023, 'eval_steps_per_second': 0.003, 'epoch': 18.0}\n",
    "\n",
    "{'eval_loss': 3.9020133018493652, 'eval_runtime': 40900.6022, 'eval_samples_per_second': 0.024, 'eval_steps_per_second': 0.003, 'epoch': 19.0}\n",
    "\n",
    "{'eval_loss': 3.8881096839904785, 'eval_runtime': 7063.4627, 'eval_samples_per_second': 0.142, 'eval_steps_per_second': 0.018, 'epoch': 20.0}\n",
    "\n",
    "{'eval_loss': 3.8538107872009277, 'eval_runtime': 1644.6757, 'eval_samples_per_second': 0.608, 'eval_steps_per_second': 0.076, 'epoch': 21.0}\n",
    "\n",
    "{'eval_loss': 3.8328824043273926, 'eval_runtime': 929.659, 'eval_samples_per_second': 1.076, 'eval_steps_per_second': 0.134, 'epoch': 22.0}\n",
    "\n",
    "{'eval_loss': 3.8504555225372314, 'eval_runtime': 936.0095, 'eval_samples_per_second': 1.068, 'eval_steps_per_second': 0.134, 'epoch': 23.0}\n",
    "\n",
    "{'eval_loss': 3.7956485748291016, 'eval_runtime': 1271.7185, 'eval_samples_per_second': 0.786, 'eval_steps_per_second': 0.098, 'epoch': 24.0}\n",
    "\n",
    "{'eval_loss': 3.826507329940796, 'eval_runtime': 847.2075, 'eval_samples_per_second': 1.18, 'eval_steps_per_second': 0.148, 'epoch': 25.0}\n",
    "\n",
    "{'train_runtime': 446096.5143, 'train_samples_per_second': 0.056, 'train_steps_per_second': 0.014, 'train_loss': 4.104190927734375, 'epoch': 25.0}\n",
    "\n",
    "Step 5. End Train 2021-11-29 20:49:15.124226\n",
    "\n",
    "Step 6. End Evaluate 2021-11-29 21:02:17.346555\n",
    "\n",
    "Step 7. Save trained model 2021-11-29 21:02:17.347608\n",
    "\n",
    "Saving model checkpoint to D:/ml/malayalam_2020_wiki\n",
    "Configuration saved in D:/ml/malayalam_2020_wiki\\config.json\n",
    "Model weights saved in D:/ml/malayalam_2020_wiki\\pytorch_model.bin\n",
    "\n",
    "Step 8. End of Model Save 2021-11-29 21:02:17.658706"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a6d80f51f506134544531300b7f5922c26327d24a4b84bf6b369a7635ed2154d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
